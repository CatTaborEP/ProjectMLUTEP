{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPL73KD5J2aYSiFg2s/YjOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CatTaborEP/ProjectMLUTEP/blob/main/MLProject_Take2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Tokenization and Neural Nets"
      ],
      "metadata": {
        "id": "sTI1izYRJUTI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lcy4AKsJTRR",
        "outputId": "7440733d-eb54-44ff-fe24-0bc0c9384dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import data as a dataframe\n",
        "def load_dataset(filename):\n",
        "  return pd.read_csv(filename)\n",
        "X = load_dataset('lexile.csv')\n",
        "#define a label\n",
        "Y = X['BT Easiness']\n",
        "excerpt = X['Excerpt']\n",
        "#drop unecessary columns and label to create a new dataset to work with.\n",
        "newX =X.drop(['ID','BT s.e.','British WC','Joon\\nWC v1','Sentence\\nCount v2',\n",
        "       'Flesch-Reading-Ease', 'Flesch-Kincaid-Grade-Level',\n",
        "       'Automated Readability Index', 'SMOG Readability',\n",
        "       'New Dale-Chall Readability Formula', 'CAREC', 'CAREC_M', 'CARES',\n",
        "       'CML2RI','Last Changed', 'Author','Title','MPAA\\nMax','British Words',\n",
        "       'MPAA \\n#Max', 'Anthology','URL','Source','Pub Year','Category','Location','License','Excerpt','BT Easiness','firstPlace_pred', 'secondPlace_pred', 'thirdPlace_pred',\n",
        "       'fourthPlace_pred', 'fifthPlace_pred', 'sixthPlace_pred','Kaggle split'], axis='columns')\n",
        "a =newX.columns\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WaZvrgTJo0p",
        "outputId": "d0e9021b-d5c5-40e2-c6e0-3c40efee0717"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['MPAA\\n#Avg', 'Google\\nWC', 'Sentence\\nCount v1', 'Paragraphs'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the text of all of the excerpts.  The functions need to have the text all in lower case and use all English words."
      ],
      "metadata": {
        "id": "uEMcdFz0Qv8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7a_fezDKSF_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removeContractions(phrase):\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ],
      "metadata": {
        "id": "3UsqHxLCQEMZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_function(x_train):\n",
        "    text_list = []\n",
        "    for text in tqdm(x_train.values):\n",
        "        text =removeContractions(text)\n",
        "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "        text = text.lower()\n",
        "        text = nltk.word_tokenize(text)\n",
        "        [word for word in text if not word in set(stopwords.words(\"english\"))]\n",
        "        lemmatizer = nltk.WordNetLemmatizer()\n",
        "        text = [lemmatizer.lemmatize(word) for word in text]\n",
        "        text = \" \".join(text)\n",
        "        text_list.append(text)\n",
        "    text_list = pd.Series(text_list)\n",
        "    text_list.column = ['Converted_text']\n",
        "    return text_list"
      ],
      "metadata": {
        "id": "KfBbcqF3LcpW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = preprocessing_function(excerpt)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXQiyijDME0E",
        "outputId": "fc7ba46e-b30e-401c-8487-c744531dab68"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4724/4724 [02:00<00:00, 39.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdiFHG_kQaoJ",
        "outputId": "5e500394-8ad5-4ecf-ab96-05634ee7f24c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    when the young people returned to the ballroom...\n",
              "1    all through dinner time mr fayre wa somewhat s...\n",
              "2    a roger had predicted the snow departed a quic...\n",
              "3    mr grime wa to come up next morning to sir joh...\n",
              "4    and outside before the palace a great garden w...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_scaler(data):\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(data)\n",
        "    x_scaled = scaler.transform(data)\n",
        "    return x_scaled"
      ],
      "metadata": {
        "id": "dNwpueTbSGh8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_scaled= min_max_scaler(newX)"
      ],
      "metadata": {
        "id": "1Hi7rk8YaBVX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Tfidfvectorizer(data):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    x_vectorized_train = vectorizer.fit_transform(data)\n",
        "    return x_vectorized_train"
      ],
      "metadata": {
        "id": "pJ-AMjvaSk5M"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_vectorized = Tfidfvectorizer(preprocessed_text)\n",
        "x_vectorized = x_vectorized.toarray()\n"
      ],
      "metadata": {
        "id": "T1cv3m6iVQRc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z = np.concatenate((x_vectorized, x_scaled), axis = 1)"
      ],
      "metadata": {
        "id": "KGpRuvEyaQkg"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}